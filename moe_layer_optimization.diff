--- qwen_moe_layer_original.cpp	2025-07-25 12:46:05.936130497 +0000
+++ optimized_moe_layer/qwen_moe_layer.cpp	2025-07-25 12:45:01.307801181 +0000
@@ -27,10 +27,14 @@
 #include <omp.h>
 #include <qwen_moe_layer.h>
 #include <stdexcept>
+#include <immintrin.h>  // For SIMD optimizations
+#include <algorithm>
 
 namespace causallm {
 
 static constexpr size_t SINGLE_INOUT_IDX = 0;
+// Cache line size for optimal memory access
+static constexpr size_t CACHE_LINE_SIZE = 64;
 
 MoELayer::MoELayer() :
   LayerImpl(),
@@ -138,7 +142,7 @@
       "expert_down_" + std::to_string(i), false));
   }
 
-  // 6. Request intermediate tensors
+  // 6. Request intermediate tensors with optimized sizes
   const unsigned batch_size = in_dim.batch();
   const unsigned seq_len = in_dim.height();
   const unsigned total_tokens = batch_size * seq_len;
@@ -149,11 +153,20 @@
                           nntrainer::Initializer::NONE, false,
                           nntrainer::TensorLifespan::FORWARD_FUNC_LIFESPAN);
 
-  // Expert mask: [num_experts, batch*seq]
+  // Expert mask: [num_experts, batch*seq] - optimized layout
   expert_mask_idx =
     context.requestTensor({num_experts, 1, topk, total_tokens}, "expert_mask",
                           nntrainer::Initializer::ZEROS, false,
                           nntrainer::TensorLifespan::FORWARD_FUNC_LIFESPAN);
+  
+  // Pre-allocate thread-local intermediate buffers for better performance
+  unsigned int max_threads = omp_get_max_threads();
+  thread_local_buffers.resize(max_threads);
+  for (unsigned int i = 0; i < max_threads; ++i) {
+    thread_local_buffers[i].gate_out.resize(intermediate_size);
+    thread_local_buffers[i].up_out.resize(intermediate_size);
+    thread_local_buffers[i].intermediate.resize(intermediate_size);
+  }
 }
 
 void MoELayer::forwarding(nntrainer::RunLayerContext &context, bool training) {
@@ -184,40 +197,48 @@
   auto topk_indices = std::get<1>(topk_result);
 
   const uint32_t *indices_data = topk_indices.getData<uint32_t>();
-#pragma omp parallel for collapse(2)
-  for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
-    for (int k = 0; k < static_cast<int>(topk); ++k) {
-      expert_mask.setValue(indices_data[i * topk + k], 0, k, i, 1.0f);
-    }
-  }
-
+  const float *values_data = topk_values.getData<float>();
+  
   // Pre-compute expert token assignments for better cache locality
   std::vector<std::vector<std::pair<unsigned, float>>> expert_assignments(
     num_experts);
-  for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
-    for (int k = 0; k < static_cast<int>(topk); ++k) {
-      unsigned expert_idx = indices_data[i * topk + k];
-      float weight = topk_values.getValue<float>(i, 0, 0, k);
+  
+  // Reserve space to avoid reallocations
+  for (unsigned int i = 0; i < num_experts; ++i) {
+    expert_assignments[i].reserve(total_tokens * topk / num_experts + 1);
+  }
+  
+  // Optimized assignment collection with better memory access pattern
+  for (unsigned int i = 0; i < total_tokens; ++i) {
+    const unsigned int base_idx = i * topk;
+    for (unsigned int k = 0; k < topk; ++k) {
+      unsigned expert_idx = indices_data[base_idx + k];
+      float weight = values_data[base_idx + k];
       expert_assignments[expert_idx].emplace_back(i, weight);
+      expert_mask.setValue(expert_idx, 0, k, i, 1.0f);
     }
   }
 
-// expert forwarding with optimized memory access
+// expert forwarding with optimized memory access and SIMD
 #pragma omp parallel
   {
-#pragma omp for schedule(dynamic)
+    int thread_id = omp_get_thread_num();
+    auto &buffer = thread_local_buffers[thread_id];
+    
+#pragma omp for schedule(dynamic, 1)
     for (int expert_idx = 0; expert_idx < static_cast<int>(num_experts);
          ++expert_idx) {
       const auto &assignments = expert_assignments[expert_idx];
       if (assignments.empty())
         continue;
 
-      // Use optimized expert forward computation without memory copies
-      compute_expert_forward(
+      // Use optimized expert forward computation with thread-local buffers
+      compute_expert_forward_optimized(
         input, output, assignments,
         context.getWeight(expert_gate_proj_indices[expert_idx]),
         context.getWeight(expert_up_proj_indices[expert_idx]),
-        context.getWeight(expert_down_proj_indices[expert_idx]), hidden_size);
+        context.getWeight(expert_down_proj_indices[expert_idx]), 
+        hidden_size, buffer);
     }
   }
 
@@ -225,11 +246,13 @@
   output.reshape({batch_size, 1, seq_len, hidden_size});
 }
 
-inline void MoELayer::compute_expert_forward(
+// Optimized compute function using SIMD and better memory patterns
+inline void MoELayer::compute_expert_forward_optimized(
   const nntrainer::Tensor &input, nntrainer::Tensor &output,
   const std::vector<std::pair<unsigned, float>> &token_assignments,
   const nntrainer::Tensor &gate_proj, const nntrainer::Tensor &up_proj,
-  const nntrainer::Tensor &down_proj, unsigned int hidden_size) {
+  const nntrainer::Tensor &down_proj, unsigned int hidden_size,
+  ThreadLocalBuffer &buffer) {
 
   const unsigned intermediate_size = gate_proj.width();
   const unsigned num_tokens = token_assignments.size();
@@ -237,53 +260,118 @@
   if (num_tokens == 0)
     return;
 
-  // Create tensor dimensions for single token processing
-  nntrainer::TensorDim token_input_dim({1, 1, 1, hidden_size},
-                                       input.getTensorType());
-  nntrainer::TensorDim intermediate_dim({1, 1, 1, intermediate_size},
-                                        input.getTensorType());
-  nntrainer::TensorDim token_output_dim({1, 1, 1, hidden_size},
-                                        input.getTensorType());
-
-  // Process each token individually to avoid memory copies
-  for (size_t i = 0; i < num_tokens; ++i) {
-    const unsigned token_idx = token_assignments[i].first;
-    const float weight = token_assignments[i].second;
-
-    // Create shared tensor for input token (no memory copy)
-    size_t token_offset = token_idx * hidden_size;
-    nntrainer::Tensor token_input =
-      input.getSharedDataTensor(token_input_dim, token_offset, true);
-
-    // Create intermediate tensors for this token
-    nntrainer::Tensor gate_out(intermediate_dim);
-    nntrainer::Tensor acti_out(intermediate_dim);
-    nntrainer::Tensor up_out(intermediate_dim);
-
-    // Gate projection using optimized dot operation
-    token_input.dot(gate_proj, gate_out);
-
-    // Apply activation (silu)
-    acti_func.run_fn(gate_out, acti_out);
-
-    // Up projection using optimized dot operation
-    token_input.dot(up_proj, up_out);
-
-    // Element-wise multiply: silu(gate_out) * up_out
-    acti_out.multiply_i(up_out);
-
-    // Down projection using optimized dot operation
-    nntrainer::Tensor token_expert_output(token_output_dim);
-    acti_out.dot(down_proj, token_expert_output);
-
-    // Apply weight and accumulate to final output using shared tensor
-    size_t output_offset = token_idx * hidden_size;
-    nntrainer::Tensor token_output =
-      output.getSharedDataTensor(token_output_dim, output_offset, true);
-
-    // Scale by weight and accumulate
-    token_expert_output.multiply_i(weight);
-    token_output.add_i(token_expert_output);
+  const float *gate_data = gate_proj.getData<float>();
+  const float *up_data = up_proj.getData<float>();
+  const float *down_data = down_proj.getData<float>();
+
+  // Process tokens in batches for better cache utilization
+  constexpr size_t BATCH_SIZE = 4;
+  
+  for (size_t batch_start = 0; batch_start < num_tokens; batch_start += BATCH_SIZE) {
+    size_t batch_end = std::min(batch_start + BATCH_SIZE, num_tokens);
+    
+    for (size_t i = batch_start; i < batch_end; ++i) {
+      const unsigned token_idx = token_assignments[i].first;
+      const float weight = token_assignments[i].second;
+      
+      const float *input_ptr = input.getData<float>() + token_idx * hidden_size;
+      float *output_ptr = output.getData<float>() + token_idx * hidden_size;
+      
+      // Reset buffers
+      std::fill(buffer.gate_out.begin(), buffer.gate_out.end(), 0.0f);
+      std::fill(buffer.up_out.begin(), buffer.up_out.end(), 0.0f);
+      
+      // Optimized matrix multiplication using SIMD when possible
+      // Gate projection: input @ gate_proj^T
+      optimized_gemv(input_ptr, gate_data, buffer.gate_out.data(), 
+                     hidden_size, intermediate_size);
+      
+      // Up projection: input @ up_proj^T
+      optimized_gemv(input_ptr, up_data, buffer.up_out.data(), 
+                     hidden_size, intermediate_size);
+      
+      // Apply activation (silu) and multiply with up_out
+      apply_silu_and_multiply(buffer.gate_out.data(), buffer.up_out.data(), 
+                              buffer.intermediate.data(), intermediate_size);
+      
+      // Down projection and accumulate: intermediate @ down_proj^T
+      optimized_gemv_accumulate(buffer.intermediate.data(), down_data, 
+                                output_ptr, intermediate_size, hidden_size, weight);
+    }
+  }
+}
+
+// Optimized matrix-vector multiplication
+inline void MoELayer::optimized_gemv(const float *matrix_row, const float *matrix,
+                                     float *result, size_t in_dim, size_t out_dim) {
+  // Use SIMD instructions for better performance
+  #pragma omp simd aligned(matrix_row, matrix, result : CACHE_LINE_SIZE)
+  for (size_t i = 0; i < out_dim; ++i) {
+    float sum = 0.0f;
+    const float *mat_ptr = matrix + i * in_dim;
+    
+    // Unroll loop for better performance
+    size_t j = 0;
+    for (; j + 7 < in_dim; j += 8) {
+      sum += matrix_row[j] * mat_ptr[j];
+      sum += matrix_row[j+1] * mat_ptr[j+1];
+      sum += matrix_row[j+2] * mat_ptr[j+2];
+      sum += matrix_row[j+3] * mat_ptr[j+3];
+      sum += matrix_row[j+4] * mat_ptr[j+4];
+      sum += matrix_row[j+5] * mat_ptr[j+5];
+      sum += matrix_row[j+6] * mat_ptr[j+6];
+      sum += matrix_row[j+7] * mat_ptr[j+7];
+    }
+    
+    // Handle remaining elements
+    for (; j < in_dim; ++j) {
+      sum += matrix_row[j] * mat_ptr[j];
+    }
+    
+    result[i] = sum;
+  }
+}
+
+// Optimized matrix-vector multiplication with accumulation
+inline void MoELayer::optimized_gemv_accumulate(const float *matrix_row, const float *matrix,
+                                                float *result, size_t in_dim, size_t out_dim,
+                                                float scale) {
+  #pragma omp simd aligned(matrix_row, matrix, result : CACHE_LINE_SIZE)
+  for (size_t i = 0; i < out_dim; ++i) {
+    float sum = 0.0f;
+    const float *mat_ptr = matrix + i * in_dim;
+    
+    // Unroll loop for better performance
+    size_t j = 0;
+    for (; j + 7 < in_dim; j += 8) {
+      sum += matrix_row[j] * mat_ptr[j];
+      sum += matrix_row[j+1] * mat_ptr[j+1];
+      sum += matrix_row[j+2] * mat_ptr[j+2];
+      sum += matrix_row[j+3] * mat_ptr[j+3];
+      sum += matrix_row[j+4] * mat_ptr[j+4];
+      sum += matrix_row[j+5] * mat_ptr[j+5];
+      sum += matrix_row[j+6] * mat_ptr[j+6];
+      sum += matrix_row[j+7] * mat_ptr[j+7];
+    }
+    
+    // Handle remaining elements
+    for (; j < in_dim; ++j) {
+      sum += matrix_row[j] * mat_ptr[j];
+    }
+    
+    result[i] += sum * scale;
+  }
+}
+
+// Apply SiLU activation and element-wise multiplication
+inline void MoELayer::apply_silu_and_multiply(const float *gate_out, const float *up_out,
+                                              float *result, size_t size) {
+  #pragma omp simd aligned(gate_out, up_out, result : CACHE_LINE_SIZE)
+  for (size_t i = 0; i < size; ++i) {
+    // SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))
+    float x = gate_out[i];
+    float sigmoid = 1.0f / (1.0f + expf(-x));
+    result[i] = x * sigmoid * up_out[i];
   }
 }
 
@@ -314,6 +402,9 @@
   input_step_dim.height(to - from);
   output_step_dim.height(to - from);
 
+  // Get thread-local buffer for single-threaded incremental processing
+  auto &buffer = thread_local_buffers[0];
+
   for (unsigned int b = 0; b < input_.batch(); ++b) {
 
     auto input = input_.getSharedDataTensor(
@@ -328,6 +419,89 @@
     const unsigned hidden_size = input.width();
     const unsigned total_tokens = batch_size * seq_len;
 
+    // For incremental forwarding, we typically process one token
+    // Optimize for this case
+    if (total_tokens == 1) {
+      // Direct processing without reshaping for single token
+      output.setZero();
+      expert_mask.setZero();
+
+      // Routing - optimized for single token
+      nntrainer::Tensor &gate_weights = context.getWeight(gate_idx);
+      const float *input_data = input.getData<float>();
+      float *logits_data = router_logits.getData<float>();
+      
+      // Direct matrix-vector multiplication for routing
+      optimized_gemv(input_data, gate_weights.getData<float>(), 
+                     logits_data, hidden_size, num_experts);
+      
+      // Apply softmax in-place
+      float max_val = *std::max_element(logits_data, logits_data + num_experts);
+      float sum = 0.0f;
+      for (unsigned i = 0; i < num_experts; ++i) {
+        logits_data[i] = expf(logits_data[i] - max_val);
+        sum += logits_data[i];
+      }
+      float inv_sum = 1.0f / sum;
+      for (unsigned i = 0; i < num_experts; ++i) {
+        logits_data[i] *= inv_sum;
+      }
+      
+      // Find top-k experts efficiently for single token
+      std::vector<std::pair<float, unsigned>> expert_scores;
+      expert_scores.reserve(num_experts);
+      for (unsigned i = 0; i < num_experts; ++i) {
+        expert_scores.emplace_back(logits_data[i], i);
+      }
+      
+      // Partial sort to get top-k
+      std::partial_sort(expert_scores.begin(), expert_scores.begin() + topk,
+                        expert_scores.end(), std::greater<std::pair<float, unsigned>>());
+      
+      // Normalize top-k weights
+      float topk_sum = 0.0f;
+      for (unsigned k = 0; k < topk; ++k) {
+        topk_sum += expert_scores[k].first;
+      }
+      float norm_factor = 1.0f / topk_sum;
+      
+      // Process selected experts directly
+      float *output_data = output.getData<float>();
+      for (unsigned k = 0; k < topk; ++k) {
+        unsigned expert_idx = expert_scores[k].second;
+        float weight = expert_scores[k].first * norm_factor;
+        
+        // Set expert mask
+        expert_mask.setValue(expert_idx, 0, k, 0, 1.0f);
+        
+        // Process this expert
+        const float *gate_data = context.getWeight(expert_gate_proj_indices[expert_idx]).getData<float>();
+        const float *up_data = context.getWeight(expert_up_proj_indices[expert_idx]).getData<float>();
+        const float *down_data = context.getWeight(expert_down_proj_indices[expert_idx]).getData<float>();
+        
+        const unsigned intermediate_size = context.getWeight(expert_gate_proj_indices[expert_idx]).width();
+        
+        // Gate projection
+        optimized_gemv(input_data, gate_data, buffer.gate_out.data(), 
+                       hidden_size, intermediate_size);
+        
+        // Up projection
+        optimized_gemv(input_data, up_data, buffer.up_out.data(), 
+                       hidden_size, intermediate_size);
+        
+        // Apply activation and multiply
+        apply_silu_and_multiply(buffer.gate_out.data(), buffer.up_out.data(),
+                                buffer.intermediate.data(), intermediate_size);
+        
+        // Down projection with accumulation
+        optimized_gemv_accumulate(buffer.intermediate.data(), down_data,
+                                  output_data, intermediate_size, hidden_size, weight);
+      }
+      
+      continue; // Skip the general path
+    }
+
+    // General path for multiple tokens (fallback)
     // reshape input: [B,1,S,H] -> [B*S,1,1,H]
     input.reshape({total_tokens, 1, 1, hidden_size});
 
@@ -348,20 +522,16 @@
     topk_values.divide_i(topk_values.sum(3));
 
     const uint32_t *indices_data = topk_indices.getData<uint32_t>();
-    // Set expert mask
-    for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
-      for (int k = 0; k < static_cast<int>(topk); ++k) {
-        expert_mask.setValue(indices_data[i * topk + k], 0, k, i, 1.0f);
-      }
-    }
-
-    // Pre-compute expert token assignments for better performance
+    const float *values_data = topk_values.getData<float>();
+    
+    // Set expert mask and prepare assignments
     std::vector<std::vector<std::pair<unsigned, float>>> expert_assignments(
       num_experts);
     for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
       for (int k = 0; k < static_cast<int>(topk); ++k) {
         unsigned expert_idx = indices_data[i * topk + k];
-        float weight = topk_values.getValue<float>(i, 0, 0, k);
+        float weight = values_data[i * topk + k];
+        expert_mask.setValue(expert_idx, 0, k, i, 1.0f);
         expert_assignments[expert_idx].emplace_back(i, weight);
       }
     }
@@ -373,12 +543,13 @@
       if (assignments.empty())
         continue;
 
-      // Use optimized computation for incremental forwarding
-      compute_expert_forward(
+      // Use optimized computation
+      compute_expert_forward_optimized(
         input, output, assignments,
         context.getWeight(expert_gate_proj_indices[expert_idx]),
         context.getWeight(expert_up_proj_indices[expert_idx]),
-        context.getWeight(expert_down_proj_indices[expert_idx]), hidden_size);
+        context.getWeight(expert_down_proj_indices[expert_idx]), 
+        hidden_size, buffer);
     }
 
     // reshape output: [B*S,1,1,H] -> [B,1,S,H]
@@ -386,6 +557,19 @@
   }
 }
 
+// Keep the original compute_expert_forward for compatibility
+inline void MoELayer::compute_expert_forward(
+  const nntrainer::Tensor &input, nntrainer::Tensor &output,
+  const std::vector<std::pair<unsigned, float>> &token_assignments,
+  const nntrainer::Tensor &gate_proj, const nntrainer::Tensor &up_proj,
+  const nntrainer::Tensor &down_proj, unsigned int hidden_size) {
+
+  // Use thread-local buffer 0 for backward compatibility
+  compute_expert_forward_optimized(input, output, token_assignments,
+                                   gate_proj, up_proj, down_proj,
+                                   hidden_size, thread_local_buffers[0]);
+}
+
 void MoELayer::setProperty(const std::vector<std::string> &values) {
   auto remain_props = loadProperties(values, moe_props);
   nntrainer::LayerImpl::setProperty(remain_props);
