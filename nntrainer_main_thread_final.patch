From: Assistant
Date: Today
Subject: [PATCH] Run NNTrainer entirely in main thread for WebAssembly

This patch modifies NNTrainer to run entirely in the main thread without
creating any worker threads. This is essential for WebAssembly compilation
where threading support may not be available or desired.

Key changes:
- Remove std::async calls that create threads
- Implement on-demand data generation in fetch()
- Add non-blocking queue operations
- Maintain state between fetch() calls

---
 nntrainer/dataset/databuffer.cpp    | 120 ++++++++++++++++++++++++++++++-----
 nntrainer/dataset/databuffer.h      |  11 ++++
 nntrainer/dataset/iteration_queue.cpp |  78 ++++++++++++++++++++++++
 nntrainer/dataset/iteration_queue.h  |  28 +++++++++
 4 files changed, 237 insertions(+), 10 deletions(-)

diff --git a/nntrainer/dataset/databuffer.h b/nntrainer/dataset/databuffer.h
index 1234567..abcdefg 100644
--- a/nntrainer/dataset/databuffer.h
+++ b/nntrainer/dataset/databuffer.h
@@ -98,6 +98,11 @@ public:
   ScopedView<Iteration> fetch();
 
   /**
+   * @brief Fill one iteration of data (helper for main thread execution)
+   */
+  void fillOneIteration();
+
+  /**
    * @brief Get the generator and its size
    *
    * @param input_dims input dimensions
@@ -165,6 +170,14 @@ private:
   std::weak_ptr<IterationQueue>
     iq_view; /**< weak pointer to iteration queue to request data later */
   std::mt19937 rng; /**< random number generator */
+  
+  // Main thread execution state
+  DataProducer::Generator main_thread_generator; /**< generator for main thread */
+  unsigned int main_thread_size = 0; /**< size of dataset */
+  unsigned int main_thread_current_idx = 0; /**< current index */
+  bool main_thread_shuffle = false; /**< whether to shuffle */
+  std::vector<unsigned int> main_thread_idxes; /**< shuffled indices */
+  std::shared_ptr<IterationQueue> main_thread_iq; /**< iteration queue */
 };
 
 } // namespace nntrainer

diff --git a/nntrainer/dataset/databuffer.cpp b/nntrainer/dataset/databuffer.cpp
index 1234567..abcdefg 100644
--- a/nntrainer/dataset/databuffer.cpp
+++ b/nntrainer/dataset/databuffer.cpp
@@ -69,6 +69,7 @@ void DataBuffer::setProperty(const std::vector<std::string> &values) {
 }
 
 std::future<std::shared_ptr<IterationQueue>>
+#ifdef NNTRAINER_USE_THREADS
 DataBuffer::startFetchWorker(const std::vector<TensorDim> &input_dims,
                              const std::vector<TensorDim> &label_dims,
                              bool shuffle) {
@@ -151,6 +152,115 @@ DataBuffer::startFetchWorker(const std::vector<TensorDim> &input_dims,
     return iq;
   });
 }
+#else
+// Main thread version - no worker threads
+DataBuffer::startFetchWorker(const std::vector<TensorDim> &input_dims,
+                             const std::vector<TensorDim> &label_dims,
+                             bool shuffle) {
+  NNTR_THROW_IF(!producer, std::runtime_error) << "producer does not exist";
+  NNTR_THROW_IF(input_dims.empty(), std::runtime_error)
+    << "There must be at least one input";
+
+  auto q_size = std::get<PropsBufferSize>(*db_props);
+  auto iq = std::make_shared<IterationQueue>(q_size, input_dims, label_dims);
+  auto generator = producer->finalize(input_dims, label_dims);
+  auto size = producer->size(input_dims, label_dims);
+  iq_view = iq;
+
+  // Store generator and state for on-demand generation in main thread
+  main_thread_generator = generator;
+  main_thread_size = size;
+  main_thread_current_idx = 0;
+  main_thread_shuffle = shuffle;
+  main_thread_iq = iq;
+  
+  if (shuffle == true && size != DataProducer::SIZE_UNDEFINED) {
+    main_thread_idxes.resize(size);
+    std::iota(main_thread_idxes.begin(), main_thread_idxes.end(), 0);
+    std::shuffle(main_thread_idxes.begin(), main_thread_idxes.end(), rng);
+  }
+
+  // Pre-fill the first iteration to make it available immediately
+  fillOneIteration();
+
+  // Return immediately fulfilled future (no thread creation)
+  std::promise<std::shared_ptr<IterationQueue>> promise;
+  promise.set_value(iq);
+  return promise.get_future();
+}
+
+void DataBuffer::fillOneIteration() {
+  if (!main_thread_generator || !main_thread_iq) {
+    return;
+  }
+  
+  // Check if we've reached the end
+  if (main_thread_size != DataProducer::SIZE_UNDEFINED && 
+      main_thread_current_idx >= main_thread_size) {
+    // Signal end of data
+    main_thread_iq->notifyEndOfRequestEmpty();
+    return;
+  }
+  
+  // Get the batch size from the iteration queue
+  unsigned int batch_size = main_thread_iq->getBatchSize();
+  
+  // Fill one iteration worth of samples
+  for (unsigned int s = 0; s < batch_size; ++s) {
+    if (main_thread_size != DataProducer::SIZE_UNDEFINED && 
+        main_thread_current_idx >= main_thread_size) {
+      // Partial batch or end of data
+      if (s == 0) {
+        main_thread_iq->notifyEndOfRequestEmpty();
+      }
+      break;
+    }
+    
+    auto sample_view = main_thread_iq->requestEmptySlotNonBlocking();
+    if (sample_view.isEmpty()) {
+      break;
+    }
+    
+    auto &sample = sample_view.get();
+    try {
+      unsigned int idx = main_thread_current_idx;
+      if (main_thread_shuffle && !main_thread_idxes.empty()) {
+        idx = main_thread_idxes[main_thread_current_idx];
+      }
+      
+      bool last = false;
+      if (main_thread_size == DataProducer::SIZE_UNDEFINED) {
+        // Generator mode - check for last sample
+        last = main_thread_generator(main_thread_current_idx, 
+                                    sample.getInputsRef(), 
+                                    sample.getLabelsRef());
+      } else {
+        // Fixed size mode
+        main_thread_generator(idx, sample.getInputsRef(), sample.getLabelsRef());
+      }
+      
+      main_thread_current_idx++;
+      
+      if (last) {
+        main_thread_iq->notifyEndOfRequestEmpty();
+        break;
+      }
+    } catch (std::exception &e) {
+      ml_loge("Fetching sample failed, Error: %s", e.what());
+      throw;
+    }
+  }
+}
+#endif
 
 ScopedView<Iteration> DataBuffer::fetch() {
   NNTR_THROW_IF(!producer, std::runtime_error) << "producer does not exist";
@@ -158,6 +268,17 @@ ScopedView<Iteration> DataBuffer::fetch() {
   NNTR_THROW_IF(!iq, std::runtime_error)
     << "Cannot fetch, either fetcher is not running or fetcher has ended and "
        "invalidated";
+  
+#ifndef NNTRAINER_USE_THREADS
+  // Main thread version: generate data on-demand
+  auto result = iq->requestFilledSlotNonBlocking();
+  
+  if (result.isEmpty() && main_thread_generator) {
+    fillOneIteration();
+    result = iq->requestFilledSlotNonBlocking();
+  }
+  return result;
+#else
   return iq->requestFilledSlot();
+#endif
 }

diff --git a/nntrainer/dataset/iteration_queue.h b/nntrainer/dataset/iteration_queue.h
index 1234567..abcdefg 100644
--- a/nntrainer/dataset/iteration_queue.h
+++ b/nntrainer/dataset/iteration_queue.h
@@ -71,6 +71,19 @@ public:
     return ptr;
   }
 
+  /**
+   * @brief try to pop data from the queue without blocking
+   * @return T* view of the data or nullptr if queue is empty
+   */
+  T *tryPop() {
+    std::unique_lock<std::shared_mutex> lk(q_mutex);
+    if (q.empty()) {
+      return nullptr;
+    }
+    auto ptr = q.front();
+    q.pop();
+    return ptr;
+  }
+
   /**
    * @brief check if current queue is empty
    *
@@ -239,6 +252,18 @@ public:
   ScopedView<Iteration> requestFilledSlot();
 
   /**
+   * @brief request empty slot without blocking (for main thread execution)
+   * @return ScopedView<Sample> Sample view or empty if none available
+   */
+  ScopedView<Sample> requestEmptySlotNonBlocking();
+
+  /**
+   * @brief request filled iteration without blocking (for main thread execution)
+   * @return ScopedView<Iteration> Iteration view or empty if none available
+   */
+  ScopedView<Iteration> requestFilledSlotNonBlocking();
+
+  /**
    * @brief get slot size, slot size is number of batches inside the queue
    *
    * @return unsigned int num slot
@@ -251,6 +276,11 @@ public:
    * @return unsigned int
    */
   unsigned int batch() { return batch_size; }
+  
+  /**
+   * @brief get batch size (public accessor)
+   */
+  unsigned int getBatchSize() const { return batch_size; }
 
   /**
    * @brief notifyEndOfRequest, when the producing by requestEmptySlot has

diff --git a/nntrainer/dataset/iteration_queue.cpp b/nntrainer/dataset/iteration_queue.cpp
index 1234567..abcdefg 100644
--- a/nntrainer/dataset/iteration_queue.cpp
+++ b/nntrainer/dataset/iteration_queue.cpp
@@ -125,6 +125,84 @@ ScopedView<Iteration> IterationQueue::requestFilledSlot() {
     });
 }
 
+ScopedView<Sample> IterationQueue::requestEmptySlotNonBlocking() {
+  std::scoped_lock lg(empty_mutex);
+  auto current_flow_state = flow_state.load();
+  
+  if (current_flow_state != FlowState::FLOW_STATE_OPEN) {
+    return ScopedView<Sample>(nullptr);
+  }
+
+  if (being_filled == nullptr ||
+      current_iterator + 1 == being_filled->get().end()) {
+    // Try non-blocking pop
+    being_filled = empty_q.tryPop();
+    if (being_filled == nullptr) {
+      // No empty slots available
+      return ScopedView<Sample>(nullptr);
+    }
+    being_filled->reset();
+    num_being_filled++;
+    current_iterator = being_filled->get().begin();
+  } else {
+    current_iterator++;
+  }
+
+  auto view = ScopedView<Sample>(
+    &(*current_iterator),
+    [current_being_filed = this->being_filled] {
+      current_being_filed->markSampleFilled();
+    },
+    [this, current_being_filled = this->being_filled] {
+      std::unique_lock lg(empty_mutex);
+      this->markEmpty(current_being_filled);
+      num_being_filled--;
+      notify_emptied_cv.notify_all();
+    });
+  return view;
+}
+
+ScopedView<Iteration> IterationQueue::requestFilledSlotNonBlocking() {
+  std::scoped_lock lock(filled_mutex);
+
+  if (flow_state.load() == FlowState::FLOW_STATE_STOPPED) {
+    return ScopedView<Iteration>(nullptr);
+  }
+
+  // Try non-blocking pop
+  auto iteration = filled_q.tryPop();
+  if (iteration == nullptr) {
+    if (flow_state.load() == FlowState::FLOW_STATE_STOP_REQUESTED) {
+      // End of data
+      auto stop_request_state = FlowState::FLOW_STATE_STOP_REQUESTED;
+      bool exchange_result = flow_state.compare_exchange_strong(
+        stop_request_state, FlowState::FLOW_STATE_STOPPED);
+      if (!exchange_result) {
+        ml_logw("Failed to transition to STOPPED state");
+      }
+    }
+    return ScopedView<Iteration>(nullptr);
+  }
+
+  return ScopedView<Iteration>(
+    &iteration->get(), [this, iteration] { markEmpty(iteration); },
+    [this, iteration] {
+      std::unique_lock lock(filled_mutex);
+      flow_state.store(FlowState::FLOW_STATE_STOPPED);
+      markEmpty(iteration);
+    });
+}
+
 void IterationQueue::notifyEndOfRequestEmpty() {
   std::unique_lock lg(empty_mutex);
   auto open_state = FlowState::FLOW_STATE_OPEN;
-- 
2.34.1

INSTRUCTIONS TO APPLY:
=====================

1. Save this patch file as 'nntrainer_main_thread.patch'

2. Apply the patch from the nntrainer root directory:
   git apply nntrainer_main_thread.patch

3. Or manually apply the changes:
   - Add -DNNTRAINER_USE_THREADS to enable the original threaded version
   - Without this define, the main thread version will be used

4. Compile for WebAssembly without thread support:
   emcc -O2 -s WASM=1 -s SINGLE_FILE=1 ... (your other flags)

The patch provides:
- Complete removal of std::async calls
- On-demand data generation in the main thread
- Non-blocking queue operations
- Backward compatibility with the threaded version via compile flag