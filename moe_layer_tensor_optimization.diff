--- qwen_moe_layer_original.cpp	2025-07-25 12:46:05.936130497 +0000
+++ optimized_moe_layer/qwen_moe_layer.cpp	2025-07-25 12:48:57.752841247 +0000
@@ -27,6 +27,7 @@
 #include <omp.h>
 #include <qwen_moe_layer.h>
 #include <stdexcept>
+#include <algorithm>
 
 namespace causallm {
 
@@ -138,7 +139,7 @@
       "expert_down_" + std::to_string(i), false));
   }
 
-  // 6. Request intermediate tensors
+  // 6. Request intermediate tensors with optimized sizes
   const unsigned batch_size = in_dim.batch();
   const unsigned seq_len = in_dim.height();
   const unsigned total_tokens = batch_size * seq_len;
@@ -149,11 +150,23 @@
                           nntrainer::Initializer::NONE, false,
                           nntrainer::TensorLifespan::FORWARD_FUNC_LIFESPAN);
 
-  // Expert mask: [num_experts, batch*seq]
+  // Expert mask: [num_experts, batch*seq] - optimized layout
   expert_mask_idx =
     context.requestTensor({num_experts, 1, topk, total_tokens}, "expert_mask",
                           nntrainer::Initializer::ZEROS, false,
                           nntrainer::TensorLifespan::FORWARD_FUNC_LIFESPAN);
+  
+  // Pre-allocate thread-local intermediate tensors for better performance
+  unsigned int max_threads = omp_get_max_threads();
+  thread_local_tensors.resize(max_threads);
+  
+  // Initialize tensor dimensions for thread-local tensors
+  intermediate_dim = nntrainer::TensorDim({1, 1, 1, intermediate_size}, 
+                                          nntrainer::TensorDim::TensorType(context.getFormat(),
+                                                                           context.getWeightDataType()));
+  hidden_dim = nntrainer::TensorDim({1, 1, 1, hidden_size},
+                                    nntrainer::TensorDim::TensorType(context.getFormat(),
+                                                                     context.getWeightDataType()));
 }
 
 void MoELayer::forwarding(nntrainer::RunLayerContext &context, bool training) {
@@ -184,40 +197,57 @@
   auto topk_indices = std::get<1>(topk_result);
 
   const uint32_t *indices_data = topk_indices.getData<uint32_t>();
-#pragma omp parallel for collapse(2)
-  for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
-    for (int k = 0; k < static_cast<int>(topk); ++k) {
-      expert_mask.setValue(indices_data[i * topk + k], 0, k, i, 1.0f);
-    }
-  }
-
+  const float *values_data = topk_values.getData<float>();
+  
   // Pre-compute expert token assignments for better cache locality
   std::vector<std::vector<std::pair<unsigned, float>>> expert_assignments(
     num_experts);
-  for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
-    for (int k = 0; k < static_cast<int>(topk); ++k) {
-      unsigned expert_idx = indices_data[i * topk + k];
-      float weight = topk_values.getValue<float>(i, 0, 0, k);
+  
+  // Reserve space to avoid reallocations
+  for (unsigned int i = 0; i < num_experts; ++i) {
+    expert_assignments[i].reserve(total_tokens * topk / num_experts + 1);
+  }
+  
+  // Optimized assignment collection with better memory access pattern
+  for (unsigned int i = 0; i < total_tokens; ++i) {
+    const unsigned int base_idx = i * topk;
+    for (unsigned int k = 0; k < topk; ++k) {
+      unsigned expert_idx = indices_data[base_idx + k];
+      float weight = values_data[base_idx + k];
       expert_assignments[expert_idx].emplace_back(i, weight);
+      expert_mask.setValue(expert_idx, 0, k, i, 1.0f);
     }
   }
 
 // expert forwarding with optimized memory access
 #pragma omp parallel
   {
-#pragma omp for schedule(dynamic)
+    int thread_id = omp_get_thread_num();
+    auto &tensors = thread_local_tensors[thread_id];
+    
+    // Ensure thread-local tensors are initialized
+    if (tensors.gate_out.empty()) {
+      tensors.gate_out = nntrainer::Tensor(intermediate_dim);
+      tensors.up_out = nntrainer::Tensor(intermediate_dim);
+      tensors.acti_out = nntrainer::Tensor(intermediate_dim);
+      tensors.token_input = nntrainer::Tensor(hidden_dim);
+      tensors.token_output = nntrainer::Tensor(hidden_dim);
+    }
+    
+#pragma omp for schedule(dynamic, 1)
     for (int expert_idx = 0; expert_idx < static_cast<int>(num_experts);
          ++expert_idx) {
       const auto &assignments = expert_assignments[expert_idx];
       if (assignments.empty())
         continue;
 
-      // Use optimized expert forward computation without memory copies
-      compute_expert_forward(
+      // Use optimized expert forward computation with thread-local tensors
+      compute_expert_forward_optimized(
         input, output, assignments,
         context.getWeight(expert_gate_proj_indices[expert_idx]),
         context.getWeight(expert_up_proj_indices[expert_idx]),
-        context.getWeight(expert_down_proj_indices[expert_idx]), hidden_size);
+        context.getWeight(expert_down_proj_indices[expert_idx]), 
+        hidden_size, tensors);
     }
   }
 
@@ -225,11 +255,13 @@
   output.reshape({batch_size, 1, seq_len, hidden_size});
 }
 
-inline void MoELayer::compute_expert_forward(
+// Optimized compute function using tensor operations with better memory patterns
+inline void MoELayer::compute_expert_forward_optimized(
   const nntrainer::Tensor &input, nntrainer::Tensor &output,
   const std::vector<std::pair<unsigned, float>> &token_assignments,
   const nntrainer::Tensor &gate_proj, const nntrainer::Tensor &up_proj,
-  const nntrainer::Tensor &down_proj, unsigned int hidden_size) {
+  const nntrainer::Tensor &down_proj, unsigned int hidden_size,
+  ThreadLocalTensors &tensors) {
 
   const unsigned intermediate_size = gate_proj.width();
   const unsigned num_tokens = token_assignments.size();
@@ -237,53 +269,45 @@
   if (num_tokens == 0)
     return;
 
-  // Create tensor dimensions for single token processing
-  nntrainer::TensorDim token_input_dim({1, 1, 1, hidden_size},
-                                       input.getTensorType());
-  nntrainer::TensorDim intermediate_dim({1, 1, 1, intermediate_size},
-                                        input.getTensorType());
-  nntrainer::TensorDim token_output_dim({1, 1, 1, hidden_size},
-                                        input.getTensorType());
-
-  // Process each token individually to avoid memory copies
-  for (size_t i = 0; i < num_tokens; ++i) {
-    const unsigned token_idx = token_assignments[i].first;
-    const float weight = token_assignments[i].second;
-
-    // Create shared tensor for input token (no memory copy)
-    size_t token_offset = token_idx * hidden_size;
-    nntrainer::Tensor token_input =
-      input.getSharedDataTensor(token_input_dim, token_offset, true);
-
-    // Create intermediate tensors for this token
-    nntrainer::Tensor gate_out(intermediate_dim);
-    nntrainer::Tensor acti_out(intermediate_dim);
-    nntrainer::Tensor up_out(intermediate_dim);
-
-    // Gate projection using optimized dot operation
-    token_input.dot(gate_proj, gate_out);
-
-    // Apply activation (silu)
-    acti_func.run_fn(gate_out, acti_out);
-
-    // Up projection using optimized dot operation
-    token_input.dot(up_proj, up_out);
-
-    // Element-wise multiply: silu(gate_out) * up_out
-    acti_out.multiply_i(up_out);
-
-    // Down projection using optimized dot operation
-    nntrainer::Tensor token_expert_output(token_output_dim);
-    acti_out.dot(down_proj, token_expert_output);
-
-    // Apply weight and accumulate to final output using shared tensor
-    size_t output_offset = token_idx * hidden_size;
-    nntrainer::Tensor token_output =
-      output.getSharedDataTensor(token_output_dim, output_offset, true);
-
-    // Scale by weight and accumulate
-    token_expert_output.multiply_i(weight);
-    token_output.add_i(token_expert_output);
+  // Process tokens in batches for better cache utilization
+  constexpr size_t BATCH_SIZE = 4;
+  
+  for (size_t batch_start = 0; batch_start < num_tokens; batch_start += BATCH_SIZE) {
+    size_t batch_end = std::min(batch_start + BATCH_SIZE, num_tokens);
+    
+    for (size_t i = batch_start; i < batch_end; ++i) {
+      const unsigned token_idx = token_assignments[i].first;
+      const float weight = token_assignments[i].second;
+      
+      // Create shared tensor for input token (no memory copy)
+      size_t token_offset = token_idx * hidden_size;
+      nntrainer::Tensor token_input =
+        input.getSharedDataTensor(hidden_dim, token_offset, true);
+      
+      // Gate projection using tensor dot operation
+      token_input.dot(gate_proj, tensors.gate_out);
+      
+      // Apply activation (silu)
+      acti_func.run_fn(tensors.gate_out, tensors.acti_out);
+      
+      // Up projection using tensor dot operation
+      token_input.dot(up_proj, tensors.up_out);
+      
+      // Element-wise multiply: silu(gate_out) * up_out
+      tensors.acti_out.multiply_i(tensors.up_out);
+      
+      // Down projection using tensor dot operation
+      tensors.acti_out.dot(down_proj, tensors.token_output);
+      
+      // Apply weight and accumulate to final output using shared tensor
+      size_t output_offset = token_idx * hidden_size;
+      nntrainer::Tensor token_output =
+        output.getSharedDataTensor(hidden_dim, output_offset, true);
+      
+      // Scale by weight and accumulate
+      tensors.token_output.multiply_i(weight);
+      token_output.add_i(tensors.token_output);
+    }
   }
 }
 
@@ -314,6 +338,18 @@
   input_step_dim.height(to - from);
   output_step_dim.height(to - from);
 
+  // Get thread-local tensors for single-threaded incremental processing
+  auto &tensors = thread_local_tensors[0];
+  
+  // Ensure tensors are initialized
+  if (tensors.gate_out.empty()) {
+    tensors.gate_out = nntrainer::Tensor(intermediate_dim);
+    tensors.up_out = nntrainer::Tensor(intermediate_dim);
+    tensors.acti_out = nntrainer::Tensor(intermediate_dim);
+    tensors.token_input = nntrainer::Tensor(hidden_dim);
+    tensors.token_output = nntrainer::Tensor(hidden_dim);
+  }
+
   for (unsigned int b = 0; b < input_.batch(); ++b) {
 
     auto input = input_.getSharedDataTensor(
@@ -328,6 +364,74 @@
     const unsigned hidden_size = input.width();
     const unsigned total_tokens = batch_size * seq_len;
 
+    // For incremental forwarding, we typically process one token
+    // Optimize for this case
+    if (total_tokens == 1) {
+      // Direct processing without reshaping for single token
+      output.setZero();
+      expert_mask.setZero();
+
+      // Routing - use tensor operations
+      nntrainer::Tensor &gate_weights = context.getWeight(gate_idx);
+      input.dot(gate_weights, router_logits);
+      
+      // Apply softmax
+      router_logits.apply(nntrainer::ActiFunc::softmax<float>, router_logits);
+      
+      // Get top-k using tensor operations
+      auto topk_result = router_logits.topK(topk);
+      auto topk_values = std::get<0>(topk_result);
+      auto topk_indices = std::get<1>(topk_result);
+      
+      // Normalize top-k weights
+      topk_values.divide_i(topk_values.sum(3));
+      
+      const uint32_t *indices_data = topk_indices.getData<uint32_t>();
+      const float *values_data = topk_values.getData<float>();
+      
+      // Process selected experts directly
+      for (unsigned k = 0; k < topk; ++k) {
+        unsigned expert_idx = indices_data[k];
+        float weight = values_data[k];
+        
+        // Set expert mask
+        expert_mask.setValue(expert_idx, 0, k, 0, 1.0f);
+        
+        // Process this expert using tensor operations
+        const auto &gate_proj = context.getWeight(expert_gate_proj_indices[expert_idx]);
+        const auto &up_proj = context.getWeight(expert_up_proj_indices[expert_idx]);
+        const auto &down_proj = context.getWeight(expert_down_proj_indices[expert_idx]);
+        
+        // Gate projection
+        input.dot(gate_proj, tensors.gate_out);
+        
+        // Apply activation (silu)
+        acti_func.run_fn(tensors.gate_out, tensors.acti_out);
+        
+        // Up projection
+        input.dot(up_proj, tensors.up_out);
+        
+        // Element-wise multiply
+        tensors.acti_out.multiply_i(tensors.up_out);
+        
+        // Down projection
+        nntrainer::Tensor token_expert_output(hidden_dim);
+        tensors.acti_out.dot(down_proj, token_expert_output);
+
+        // Apply weight and accumulate to final output using shared tensor
+        size_t output_offset = k * hidden_size; // Corrected offset for single token
+        nntrainer::Tensor token_output =
+          output.getSharedDataTensor(hidden_dim, output_offset, true);
+
+        // Scale by weight and accumulate
+        token_expert_output.multiply_i(weight);
+        token_output.add_i(token_expert_output);
+      }
+      
+      continue; // Skip the general path
+    }
+
+    // General path for multiple tokens (fallback)
     // reshape input: [B,1,S,H] -> [B*S,1,1,H]
     input.reshape({total_tokens, 1, 1, hidden_size});
 
@@ -348,20 +452,16 @@
     topk_values.divide_i(topk_values.sum(3));
 
     const uint32_t *indices_data = topk_indices.getData<uint32_t>();
-    // Set expert mask
-    for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
-      for (int k = 0; k < static_cast<int>(topk); ++k) {
-        expert_mask.setValue(indices_data[i * topk + k], 0, k, i, 1.0f);
-      }
-    }
-
-    // Pre-compute expert token assignments for better performance
+    const float *values_data = topk_values.getData<float>();
+    
+    // Set expert mask and prepare assignments
     std::vector<std::vector<std::pair<unsigned, float>>> expert_assignments(
       num_experts);
     for (int i = 0; i < static_cast<int>(total_tokens); ++i) {
       for (int k = 0; k < static_cast<int>(topk); ++k) {
         unsigned expert_idx = indices_data[i * topk + k];
-        float weight = topk_values.getValue<float>(i, 0, 0, k);
+        float weight = values_data[i * topk + k];
+        expert_mask.setValue(expert_idx, 0, k, i, 1.0f);
         expert_assignments[expert_idx].emplace_back(i, weight);
       }
     }
@@ -373,12 +473,13 @@
       if (assignments.empty())
         continue;
 
-      // Use optimized computation for incremental forwarding
-      compute_expert_forward(
+      // Use optimized computation
+      compute_expert_forward_optimized(
         input, output, assignments,
         context.getWeight(expert_gate_proj_indices[expert_idx]),
         context.getWeight(expert_up_proj_indices[expert_idx]),
-        context.getWeight(expert_down_proj_indices[expert_idx]), hidden_size);
+        context.getWeight(expert_down_proj_indices[expert_idx]), 
+        hidden_size, tensors);
     }
 
     // reshape output: [B*S,1,1,H] -> [B,1,S,H]
@@ -386,6 +487,69 @@
   }
 }
 
+// Keep the original compute_expert_forward for compatibility
+inline void MoELayer::compute_expert_forward(
+  const nntrainer::Tensor &input, nntrainer::Tensor &output,
+  const std::vector<std::pair<unsigned, float>> &token_assignments,
+  const nntrainer::Tensor &gate_proj, const nntrainer::Tensor &up_proj,
+  const nntrainer::Tensor &down_proj, unsigned int hidden_size) {
+
+  const unsigned intermediate_size = gate_proj.width();
+  const unsigned num_tokens = token_assignments.size();
+
+  if (num_tokens == 0)
+    return;
+
+  // Create tensor dimensions for single token processing
+  nntrainer::TensorDim token_input_dim({1, 1, 1, hidden_size},
+                                       input.getTensorType());
+  nntrainer::TensorDim intermediate_dim({1, 1, 1, intermediate_size},
+                                        input.getTensorType());
+  nntrainer::TensorDim token_output_dim({1, 1, 1, hidden_size},
+                                        input.getTensorType());
+
+  // Process each token individually to avoid memory copies
+  for (size_t i = 0; i < num_tokens; ++i) {
+    const unsigned token_idx = token_assignments[i].first;
+    const float weight = token_assignments[i].second;
+
+    // Create shared tensor for input token (no memory copy)
+    size_t token_offset = token_idx * hidden_size;
+    nntrainer::Tensor token_input =
+      input.getSharedDataTensor(token_input_dim, token_offset, true);
+
+    // Create intermediate tensors for this token
+    nntrainer::Tensor gate_out(intermediate_dim);
+    nntrainer::Tensor acti_out(intermediate_dim);
+    nntrainer::Tensor up_out(intermediate_dim);
+
+    // Gate projection using optimized dot operation
+    token_input.dot(gate_proj, gate_out);
+
+    // Apply activation (silu)
+    acti_func.run_fn(gate_out, acti_out);
+
+    // Up projection using optimized dot operation
+    token_input.dot(up_proj, up_out);
+
+    // Element-wise multiply: silu(gate_out) * up_out
+    acti_out.multiply_i(up_out);
+
+    // Down projection using optimized dot operation
+    nntrainer::Tensor token_expert_output(token_output_dim);
+    acti_out.dot(down_proj, token_expert_output);
+
+    // Apply weight and accumulate to final output using shared tensor
+    size_t output_offset = token_idx * hidden_size;
+    nntrainer::Tensor token_output =
+      output.getSharedDataTensor(token_output_dim, output_offset, true);
+
+    // Scale by weight and accumulate
+    token_expert_output.multiply_i(weight);
+    token_output.add_i(token_expert_output);
+  }
+}
+
 void MoELayer::setProperty(const std::vector<std::string> &values) {
   auto remain_props = loadProperties(values, moe_props);
   nntrainer::LayerImpl::setProperty(remain_props);
