--- qwen_moe_layer_original.h	2025-07-25 12:46:23.540233798 +0000
+++ optimized_moe_layer/qwen_moe_layer.h	2025-07-25 12:45:01.307801181 +0000
@@ -23,6 +23,7 @@
 #include <acti_func.h>
 #include <common_properties.h>
 #include <layer_impl.h>
+#include <vector>
 
 namespace causallm {
 
@@ -135,6 +136,15 @@
   static constexpr const char *type = "qwen_moe"; /**< type of the layer */
 
 private:
+  /**
+   * @brief Thread-local buffer for intermediate computations
+   */
+  struct ThreadLocalBuffer {
+    std::vector<float> gate_out;
+    std::vector<float> up_out;
+    std::vector<float> intermediate;
+  };
+
   unsigned int num_experts;      /**< number of experts */
   unsigned int topk;             /**< number of experts per token, i.e., topk */
   nntrainer::ActiFunc acti_func; /**< activation function for the expert */
@@ -152,6 +162,9 @@
   unsigned int router_logits_idx;
   unsigned int expert_mask_idx;
 
+  // Thread-local buffers for parallel processing
+  std::vector<ThreadLocalBuffer> thread_local_buffers;
+
   /**
    * @brief expert forward computation without memory copies
    * @param input Input tensor (reshaped to [total_tokens, 1, 1, hidden_size])
@@ -168,6 +181,58 @@
     const std::vector<std::pair<unsigned, float>> &token_assignments,
     const nntrainer::Tensor &gate_proj, const nntrainer::Tensor &up_proj,
     const nntrainer::Tensor &down_proj, unsigned int hidden_size);
+
+  /**
+   * @brief Optimized expert forward computation using SIMD and thread-local buffers
+   * @param input Input tensor (reshaped to [total_tokens, 1, 1, hidden_size])
+   * @param output Output tensor to accumulate results
+   * @param token_assignments Vector of (token_index, weight) pairs for this expert
+   * @param gate_proj Gate projection weight tensor
+   * @param up_proj Up projection weight tensor
+   * @param down_proj Down projection weight tensor
+   * @param hidden_size Hidden dimension size
+   * @param buffer Thread-local buffer for intermediate computations
+   */
+  inline void compute_expert_forward_optimized(
+    const nntrainer::Tensor &input, nntrainer::Tensor &output,
+    const std::vector<std::pair<unsigned, float>> &token_assignments,
+    const nntrainer::Tensor &gate_proj, const nntrainer::Tensor &up_proj,
+    const nntrainer::Tensor &down_proj, unsigned int hidden_size,
+    ThreadLocalBuffer &buffer);
+
+  /**
+   * @brief Optimized matrix-vector multiplication
+   * @param matrix_row Input vector
+   * @param matrix Matrix data (row-major)
+   * @param result Output vector
+   * @param in_dim Input dimension
+   * @param out_dim Output dimension
+   */
+  inline void optimized_gemv(const float *matrix_row, const float *matrix,
+                             float *result, size_t in_dim, size_t out_dim);
+
+  /**
+   * @brief Optimized matrix-vector multiplication with accumulation
+   * @param matrix_row Input vector
+   * @param matrix Matrix data (row-major)
+   * @param result Output vector (accumulated)
+   * @param in_dim Input dimension
+   * @param out_dim Output dimension
+   * @param scale Scaling factor
+   */
+  inline void optimized_gemv_accumulate(const float *matrix_row, const float *matrix,
+                                        float *result, size_t in_dim, size_t out_dim,
+                                        float scale);
+
+  /**
+   * @brief Apply SiLU activation and element-wise multiplication
+   * @param gate_out Gate output
+   * @param up_out Up projection output
+   * @param result Result of silu(gate_out) * up_out
+   * @param size Vector size
+   */
+  inline void apply_silu_and_multiply(const float *gate_out, const float *up_out,
+                                      float *result, size_t size);
 };
 } // namespace causallm
 
